{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikserrano/Programs/miniconda3/envs/cfret-map/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "from copairs import map\n",
    "from pycytominer.cyto_utils import load_profiles\n",
    "from pycytominer import annotate\n",
    "from tqdm import TqdmWarning\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from src import io_utils, data_utils\n",
    "\n",
    "# removing warnigns \n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up the necessary file paths and directories required for the notebook, ensuring that input files exist. \n",
    "It also creates a results folder if it doesn't already exist to store outputs generated during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the base data directory and ensure it exists (raises an error if it doesn't)\n",
    "data_dir = pathlib.Path(\"../data/\").resolve(strict=True)\n",
    "\n",
    "# Setting the metadata directory for updated plate maps and ensure it exists\n",
    "metadata_dir = pathlib.Path(\"../data/metadata/updated_platemaps\").resolve(strict=True)\n",
    "\n",
    "# Path to the updated barcode plate map file, ensure it exists\n",
    "platemap_path = (metadata_dir / \"updated_barcode_platemap.csv\").resolve(strict=True)\n",
    "\n",
    "# Path to the configuration file (does not enforce existence check here)\n",
    "config_path = pathlib.Path(\"../config.yaml\").resolve(strict=True)\n",
    "\n",
    "# Setting the results directory, resolve the full path, and create it if it doesn't already exist\n",
    "results_dir = pathlib.Path(\"./results\").resolve()\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading config and general configs\n",
    "configs = io_utils.load_config(config_path)\n",
    "general_configs = configs[\"general_configs\"]\n",
    "\n",
    "# loading bar code\n",
    "barcode = pd.read_csv(platemap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these files have undergone feature selection, it is essential to identify the overlapping feature names to ensure accurate and consistent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_cols = None\n",
    "for aggregated_profile in list(data_dir.glob(\"*.parquet\")):\n",
    "    # read aggreagated profiled and column names\n",
    "    agg_df = pd.read_parquet(aggregated_profile)\n",
    "    columns = list(agg_df.columns)\n",
    "\n",
    "    # Update the shared_columns set\n",
    "    if shared_cols is None:\n",
    "        # Initialize shared columns with the first profile's columns, preserving order\n",
    "        shared_cols = columns\n",
    "    else:\n",
    "        # Retain only the columns present in both the current profile and shared columns\n",
    "        shared_cols = [col for col in shared_cols if col in columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the code processes and organizes data by grouping related files and enriching them with additional metadata. Each group is assigned a unique identifier, and the corresponding data files are systematically loaded and prepared. New metadata columns are generated by combining existing information to ensure consistency and clarity. Additional metadata is integrated into the data to provide valuable experimental context, while unique identifiers are added to distinguish the aggregated profiles from different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suffix for aggregated profiles\n",
    "aggregated_file_suffix = \"aggregated.parquet\"\n",
    "\n",
    "# Dictionary to store loaded plate data grouped by batch\n",
    "loaded_plate_batches = {}\n",
    "\n",
    "# Iterate over unique platemap files and their associated plates\n",
    "for batch_index, (platemap_filename, associated_plates_df) in enumerate(\n",
    "    barcode.groupby(\"platemap_file\")\n",
    "):\n",
    "    # Generate a unique batch ID\n",
    "    batch_id = f\"batch_{batch_index + 1}\"\n",
    "\n",
    "    # Load the platemap CSV file\n",
    "    platemap_path = (metadata_dir / f\"{platemap_filename}.csv\").resolve(strict=True)\n",
    "    platemap_data = pd.read_csv(platemap_path)\n",
    "\n",
    "    # Extract all plate names associated with the current platemap\n",
    "    plate_barcodes = associated_plates_df[\"plate_barcode\"].tolist()\n",
    "\n",
    "    # List to store all loaded and processed aggregated plates for the current batch\n",
    "    loaded_aggregated_plates = []\n",
    "\n",
    "    for plate_barcode in plate_barcodes:\n",
    "        # Resolve the file path for the aggregated plate data\n",
    "        plate_file_path = (\n",
    "            data_dir / f\"{plate_barcode}_{aggregated_file_suffix}\"\n",
    "        ).resolve(strict=True)\n",
    "\n",
    "        # Load the aggregated profile data for the current plate\n",
    "        aggregated_data = load_profiles(plate_file_path)\n",
    "\n",
    "        # Create a new metadata column called 'Metadata_well' by concatenating well row and column\n",
    "        # Zero-pad single-digit well column numbers for consistency (e.g., \"B2\" -> \"B02\")\n",
    "        aggregated_data[\"Metadata_well\"] = aggregated_data[\"Metadata_WellRow\"].astype(\n",
    "            str\n",
    "        ) + aggregated_data[\"Metadata_WellCol\"].astype(int).apply(lambda x: f\"{x:02}\")\n",
    "\n",
    "        # Reorder columns to place 'Metadata_well' as the first column\n",
    "        aggregated_data = aggregated_data[\n",
    "            [\"Metadata_well\"]\n",
    "            + [col for col in aggregated_data.columns if col != \"Metadata_well\"]\n",
    "        ]\n",
    "\n",
    "        # Annotate the aggregated profile with metadata from the platemap\n",
    "        aggregated_data = annotate(\n",
    "            aggregated_data,\n",
    "            platemap_data,\n",
    "            join_on=[\"Metadata_well_position\", \"Metadata_well\"],\n",
    "        )\n",
    "\n",
    "        # split metadata features and morphology features\n",
    "        meta, _ = data_utils.split_meta_and_features(profile=aggregated_data)\n",
    "\n",
    "        # update aggregated_data with only shared features\n",
    "        aggregated_data = aggregated_data[meta + shared_cols]\n",
    "\n",
    "        # Add a new column indicating the source plate for each row\n",
    "        aggregated_data.insert(0, \"Metadata_plate_barcode\", plate_barcode)\n",
    "\n",
    "        # Append the processed aggregated data for this plate to the batch list\n",
    "        loaded_aggregated_plates.append(aggregated_data)\n",
    "\n",
    "    # Combine all processed plates for the current batch into a single DataFrame\n",
    "    combined_aggregated_data = pd.concat(loaded_aggregated_plates)\n",
    "\n",
    "    # Store the combined DataFrame in the loaded_plate_batches dictionary\n",
    "    loaded_plate_batches[batch_id] = combined_aggregated_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we analyze a high-content screening dataset generated from cell painting experiments, where failing cardiac fibroblasts are treated with multiple compounds. Our goal is to calculate the mean average precision (mAP) by comparing the experimental treatments to two controls: a negative control consisting of DMSO-treated failing cardiac fibroblasts and a positive control consisting of DMSO-treated healthy cardiac fibroblasts.\n",
    "\n",
    "We start by preparing the dataset, copying the profiles, and assigning a reference index to ensure proper grouping of non-DMSO treatment replicates. Metadata and feature columns are separated to facilitate the calculation of average precision (AP) scores. To calculate these scores, we define positive pairs as treatments with the same metadata values (e.g., same treatment type) across all plates. Negative pairs, on the other hand, are determined by comparing all DMSO-treated wells across all plates with all other treatments.\n",
    "\n",
    "Once the AP scores are computed, we aggregate them across all plates for each treatment to derive the mean average precision (mAP) score. This process captures the consistency of treatment performance relative to the controls and allows for a comprehensive evaluation of the dataset. Finally, we save both the AP and mAP scores for each control condition, providing a well-structured dataset for further interpretation and downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "# Load configurations for average precision (AP) and mean average precision (mAP)\n",
    "copairs_ap_configs = configs[\"copairs_ap_configs\"]\n",
    "copairs_map_configs = configs[\"copairs_map_configs\"]\n",
    "\n",
    "# Define control conditions for the analysis\n",
    "# Each tuple specifies the control type, treatment, and associated cell state\n",
    "control_list = [(\"negative\", \"DMSO\", \"failing\"), (\"positive\", \"DMSO\", \"healthy\")]\n",
    "\n",
    "# Iterate over each batch of loaded plate profiles\n",
    "for batch_id, profile in loaded_plate_batches.items():\n",
    "    # Analyze the profile for each control condition\n",
    "    for control_type, control_treatment, cell_state in control_list:\n",
    "        # Create a copy of the profile to preserve the original data\n",
    "        profile = profile.copy()\n",
    "\n",
    "        # Assign a default reference index based on the row index\n",
    "        profile[\"Metadata_reference_index\"] = profile.index\n",
    "\n",
    "        # Mark all non-control replicates (e.g., treatments not matching the current control)\n",
    "        profile.loc[\n",
    "            (profile[\"Metadata_treatment\"] != control_treatment)\n",
    "            & (profile[\"Metadata_cell_type\"] != cell_state),\n",
    "            \"Metadata_reference_index\",\n",
    "        ] = -1\n",
    "\n",
    "        # Move the \"Metadata_reference_index\" column to the beginning for clarity\n",
    "        profile.insert(\n",
    "            0, \"Metadata_reference_index\", profile.pop(\"Metadata_reference_index\")\n",
    "        )\n",
    "\n",
    "        # Separate metadata columns from feature columns for downstream calculations\n",
    "        meta_columns, feature_columns = data_utils.split_meta_and_features(profile)\n",
    "\n",
    "        # Calculate average precision (AP) for the profile\n",
    "        # Positive pairs are based on treatments with the same metadata\n",
    "        # Negative pairs compare DMSO-treated wells to all treatments\n",
    "        replicate_aps = map.average_precision(\n",
    "            meta=profile[meta_columns],\n",
    "            feats=profile[feature_columns].values,\n",
    "            pos_sameby=copairs_ap_configs[\"pos_sameby\"],\n",
    "            pos_diffby=copairs_ap_configs[\"pos_diffby\"],\n",
    "            neg_sameby=[],\n",
    "            neg_diffby=copairs_ap_configs[\"neg_diffby\"],\n",
    "        )\n",
    "\n",
    "        # Remove duplicate columns from the results, if any\n",
    "        replicate_aps = replicate_aps.loc[:, ~replicate_aps.columns.duplicated()]\n",
    "\n",
    "        # Exclude wells treated with the control treatment (DMSO)\n",
    "        replicate_aps = replicate_aps.loc[\n",
    "            replicate_aps[\"Metadata_treatment\"] != control_treatment\n",
    "        ]\n",
    "\n",
    "        # Save the calculated AP scores to a file for further analysis\n",
    "        replicate_aps.to_csv(\n",
    "            results_dir / f\"{control_type}_control_{cell_state}_{control_treatment}_AP_scores.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        # Calculate mean average precision (mAP) from the AP scores\n",
    "        replicate_maps = map.mean_average_precision(\n",
    "            replicate_aps,\n",
    "            sameby=copairs_map_configs[\"same_by\"],  # Grouping criteria for mAP\n",
    "            null_size=copairs_map_configs[\"null_size\"],  # Null distribution size\n",
    "            threshold=copairs_map_configs[\"threshold\"],  # Significance threshold\n",
    "            seed=general_configs[\"seed\"],  # Seed for reproducibility\n",
    "        )\n",
    "\n",
    "        # Save the mAP scores to a file for reporting\n",
    "        replicate_maps.to_csv(\n",
    "            results_dir / f\"{control_type}_control_{cell_state}_{control_treatment}_mAP_scores.csv\",\n",
    "            index=False,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfret-map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
